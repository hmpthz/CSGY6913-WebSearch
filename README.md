## Web Search Engine Lecture Final Project

Final project topics: index quantization, based on assignment 2 and assignment 3.

Please see [report.md](https://github.com/hmpthz/WebSearch_Final/blob/master/doc/report.md) or [report.pdf](https://github.com/hmpthz/WebSearch_Final/blob/master/doc/report.pdf) for complete project report.

### Assignment 2

In this assignment, you are asked to write a program that creates an inverted index structure from a set of downloaded web pages. Since the crawlers from the previous assignment are fairly slow, you will be provided a larger set of pages that you can use for this assignment; see the course web page. The data will be provided in the next few days.

In the following, we will discuss the requirements for your program. Note that the data may be larger than the memory size, and thus you MUST use I/O-efficient algorithms for index construction. Also, the data may be provided in a fairly raw form with various errors (truncated web pages, 404 errors) that you need to be able to to deal with. Here are some suggestions and requirements:

- **(0) Languages**: It is recommended that you use C/C++ or Java for this project, but you may also ask for permission to use another language. You may also use a mix of languages. Make sure you know how to “operate on a binary level” when it comes to input, output, or compression in your language. Do not build an index by simply shoving all the posting data into one of the high level data structures provided by many languages! Also, do not load the posting data into a relational database.

- **(1) Disk-based Index Structures**: You need to create an inverted index structure, plus structures for the lexicon and for the page (or docID-to-URL) table. At the end of the program, all structures need to be stored on disk in some suitable format. You may use either binary or ASCII data formats for these structures, though for full credit your diskbased inverted list structures should be in binary format at the end. It is recommended to add at least some basic index compression method to decrease the index size, but this will only be required for the next assignment (which builds on this one). Ideally, your program should have a compile flag that allows you to use ASCII format during debugging and binary format for performance.

- **(2) Problem Decomposition**: It is strongly recommended to implement this homework as 2 or 3 components, say one for generating intermediate postings from the documents and writing these postings out in unsorted or partially sorted form, one for sorting or merging the postings, and one for reformatting the sorted postings into the final index and lookup structures (though this last part can be combined with the sort or merge for best performance, either explicitly or via use of pipes and standard I/O in Unix). You may use the Unix sort utility for the middle step, you can use parts of the code for merge sort provided on the course site, or you can implement your own I/O-efficient sorting procedure. But it has to be I/O-efficient, i.e., run fast even when main memory is much smaller than the data set. (It should be possible to limit the maximum amount of memory that your program will use through some parameter setting, and your program should work on data sets of multiple GB as long as it has at least a certain amount, say a few hundred MB, of memory.)

- **(3) HTML Parsing**: To parse HTML pages, you should use a suitable parsing library. The output from this parser then has to be converted into the intermediate posting format and written to disk so that you can sort the postings afterwards. Do not spend time trying to design your own parser – that is not the objective of this assignment! Use suitable parsing libraries.

- **(4) Data Format**: Information on the input data will be provided soon in one or two formats. While you may uncompress the data before starting your program, it is preferable to uncompress during parsing, by loading one compressed file into memory and then calling the right library function to uncompress it into another memory-based buffer. Each file contains many pages.

- **(5) Index File Format**: Your inverted index must be structured such that you can read a particular inverted list without reading the rest of the index, by looking up the start of the inverted list in the lexicon structure, and then seeking forward in the file. (Figure out how to perform random seeks on disk in your chosen programming language! Do not try to skip lines in an ascii file, but use byte offsets.) The index you build should consist of three files, one for the inverted index, one for the lexicon, and one for the page table – do not use a separate file for each inverted list! Try to find a reasonably space-efficient representation for the inverted lists based on var-byte compression or some other technique. For extra credit, you may try to keep postings in compressed format on disk even during the index building operation (i.e, the temporary files are compressed and then read in and written out in compressed form during the merge – of course, this means you would not be able to easily use Unix sort for this). Do not use generic file compression techniques such as gzip or bzip2 to compress inverted lists!

- **(6) DocIDs and Term IDs**: It is recommended to assign docIDs in the order in which the pages are parsed. You may either use term IDs in the intermediate posting format, or keep the terms in textual format. Of course, the final inverted lists should not have terms or term IDs inside each posting anymore.

- **(7) Unicode**: Note that the input may contain text from many different languages, including text that uses non-ascii character sets. Read up on how to process data using unicode instead of basic ascii.

- **(8) Future Assignments**: Make sure your code can be maintained and extended easily, as the next assignment will build further on this code base.

### Assignment 3

In this assignment, you are asked to write a program that uses the inverted index structure from the second homework to answer queries typed in by a user. Please use the same data set as for HW2. In the following, I will discuss the minimum requirements for your program, and also point out some opportunities for extra credit.

- **(0) Languages**: It is recommended to use C/C++ or Java for this assignment. Or contact me if you prefer to use some other language. (If I have already given you permission to use another language for the second homework, then there is no need to ask me again.) But efficiency is very important. Even on the whole document set, your query processor should provide results in at most one or two seconds for typical queries, especially in the conjunctive case.

- **(1) Query Execution**: Your program should execute queries by using something similar to the simple interface presented in class based on operations openList(), closeList(), nextGEQ(), and getFreq() or getPayload() on inverted lists. (Recall that nextGEQ(l, id) returns the first posting in an already opened list l with docID at least id, when searching in a forward direction from the current position of the list pointer.) This way, issues such as file input and inverted list compression technique should be completely hidden from the higher-level query processor.

- **(2) Ranked Queries**: Your query execution program should compute ranked queries according to the BM25 measure presented in class. Return the top 10 or 20 results according to the scoring function. You should implement conjunctive and disjunctive queries. You may experiment with additional ranking factors (e.g., context, proximity, Pagerank) for extra credit, as optional features. For each top result, you should also return its score according to the ranking function, as well as the frequency of each search term in the document. If you use additional factors such as Pagerank, also output those scores in a suitable way.

- **(3) Interface**: Your program may read user queries via simple command line prompt, and you need to return the URL of each result, its BM25 score, and some snippet text with the context of the term occurrences in the pages. For extra credit, you could make your program web accessible, or work on smarter snippet generation algorithms, but this is not required.

- **(4) Startup**: Upon startup, your query processor may read the complete lexicon and URL table data structures from disk into main memory, which might take some seconds or even a minute. However, you should not read the inverted index itself into memory! After the user inputs a query, your program should then perform seeks on disk in order to read only those inverted lists from disk that correspond to query words, and then compute the result. After returning the result, your program should wait for the next query to be input. You could optionally implement caching of inverted list data in main memory, but you should not assume that the entire index fits in main memory. Instead, upon starting the query processor, it should be possible to select the amount of memory that is used for index caching.

- **(5) Index Compression**: Make sure that your inverted index is in binary format, and use some suitable form of index compression, such as Variable-Byte, Simple9, or another method. Do not use standard compressors such as gzip for index compression! It is recommended to use a block-wise compression scheme that allows skipping of blocks during query processing. When a query arrives, do not completely uncompress and temporarily store the complete inverted lists for the query terms – you should only decompress index data as needed inside the nextGEQ() and getFreq() operations.

### Final: Index Quantization

As discussed in class, many search systems store precomputed quantized index scores inside the inverted index postings. These are approximations of the BM25 scores that the postings, typically rounded to a precision of about 8 bits. Quantized index structures tend to result in significantly faster query processing as no BM25 scores need to be computed during the query.
While there has been some previous published work on quantization, there seems to be no recent work that compares different ways of quantizing the impact scores in terms of their impact on index size and result quality. For example, quantization could be done in a linear manner, or exponentially, or by aiming for more precision for certain ranges of scores. We could quantize to 8 bits, try to use even fewer bits, or use a variable-length representation or compress the quantized scores. We could also selectively quantize only parts of the inverted index. The goal of this course project would be to explore these options and perform a comparison, using either your own system, the [PISA system](http://engineering.nyu.edu/~suel/papers/pisa-ossirc19.pdf), or maybe a system such as Lucene.